{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ACER\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1212: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from normalizationModul5 import normalize_corpus\n",
    "from utilsModul5 import build_feature_matrix\n",
    "\n",
    "import nltk\n",
    "import string\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from gensim import models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>waktu</th>\n",
       "      <th>tweets</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-11-03 23:40:24</td>\n",
       "      <td>@AgisniNina @rmiryanti Tetap #01JokowiLagi #01...</td>\n",
       "      <td>negatif</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018-11-03 18:02:36</td>\n",
       "      <td>Ya, usia nggak ada yang tahu. Setidaknya kita ...</td>\n",
       "      <td>positif</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018-11-03 17:58:33</td>\n",
       "      <td>Saya menjual Jaket Hoodie Asian Games 2018. ma...</td>\n",
       "      <td>negatif</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018-11-03 17:50:31</td>\n",
       "      <td>Bayangin saja dalam waktu 90 detik kamu harus ...</td>\n",
       "      <td>positif</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018-11-03 17:42:47</td>\n",
       "      <td>Duka mendalam atas meninggalnya Syahrul Anto, ...</td>\n",
       "      <td>positif</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 waktu                                             tweets  \\\n",
       "0  2018-11-03 23:40:24  @AgisniNina @rmiryanti Tetap #01JokowiLagi #01...   \n",
       "1  2018-11-03 18:02:36  Ya, usia nggak ada yang tahu. Setidaknya kita ...   \n",
       "2  2018-11-03 17:58:33  Saya menjual Jaket Hoodie Asian Games 2018. ma...   \n",
       "3  2018-11-03 17:50:31  Bayangin saja dalam waktu 90 detik kamu harus ...   \n",
       "4  2018-11-03 17:42:47  Duka mendalam atas meninggalnya Syahrul Anto, ...   \n",
       "\n",
       "     label  \n",
       "0  negatif  \n",
       "1  positif  \n",
       "2  negatif  \n",
       "3  positif  \n",
       "4  positif  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.read_csv('pesawatlionair.csv')\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1    Ya, usia nggak ada yang tahu. Setidaknya kita ...\n",
      "2    Saya menjual Jaket Hoodie Asian Games 2018. ma...\n",
      "3    Bayangin saja dalam waktu 90 detik kamu harus ...\n",
      "4    Duka mendalam atas meninggalnya Syahrul Anto, ...\n",
      "Name: tweets, dtype: object\n",
      "-----------------------------\n",
      "1    positif\n",
      "2    negatif\n",
      "3    positif\n",
      "4    positif\n",
      "Name: label, dtype: object\n"
     ]
    }
   ],
   "source": [
    "feature = dataset.iloc[:,1]\n",
    "label = dataset.iloc[:,2]\n",
    "print(feature[1:5])\n",
    "print(\"-----------------------------\")\n",
    "print(label[1:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prepare_datasets(corpus, labels, test_data_proportion=0.3):\n",
    "    train_X, test_X, train_Y, test_Y = train_test_split(corpus, labels,\n",
    "                                                       test_size=0.33, random_state=42)\n",
    "    return train_X, test_X, train_Y, test_Y\n",
    "\n",
    "def remove_empty_docs(corpus, labels):\n",
    "    filtered_corpus = []\n",
    "    filtered_labels = []\n",
    "    for doc, label in zip(corpus, labels):\n",
    "        if doc.strip():\n",
    "            filtered_corpus.append(doc)\n",
    "            filtered_labels.append(label)\n",
    "    return filtered_corpus, filtered_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ACER\\Anaconda3\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.cross_validation import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_corpus, test_corpus, train_labels, test_labels = prepare_datasets(feature,\n",
    "                                                                       label,\n",
    "                                                                       test_data_proportion=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "879     Turut bersimpati atas musibah penerbangan Lion...\n",
       "239     Lion Air telah memesan pesawat Boeing 737 Max ...\n",
       "361     Ucapan takziah diucapkan kepada keluarga mangs...\n",
       "211     Selamat jalan sahabat, beristirahatlah dalam d...\n",
       "788     teruuuss...dgn entengnya kau buat status..klu ...\n",
       "445     Para delegasi yang menghadiri acara Konferensi...\n",
       "530     Semoga Allah mudahkan urusan balik semenanjung...\n",
       "227     Kemendagri Mudahkan Pengurusan Akta Kematian K...\n",
       "941     Presiden telah menginstruksikan Kepala BNPP be...\n",
       "572     Semoga semuanya segera ditemukan #LionAir #Pra...\n",
       "423     Satu Jenazah Korban Lion Air Teridentifikasi, ...\n",
       "874     Sulitnya Pencarian Badan Lion Air JT 610 Didug...\n",
       "177         semoga tabir segera tersingkap #PrayForJT610 \n",
       "1026    #PrayForJT610 Pesawat Lion Air JT 610 jatuh di...\n",
       "895     Rencananya, malam ini Persib akan bertanding m...\n",
       "449     Panglima TNI Marsekal Hadi Tjahjanto mengataka...\n",
       "543     Pesawat JT-610 yang jatuh mempunyai Certificat...\n",
       "774     Ya Alloh... semua akan kembali kepadaMU, Lion ...\n",
       "980     Pak @jokowi pun menunduk. Mendengarkan setiap ...\n",
       "531     #PrayForJT610 ... Kami turut prihatin atas mus...\n",
       "7       Visor Windshield Honda Vario 125 Pgm Fi Malam ...\n",
       "155     Sangat melelahkan, Sekarang aku menyadari beke...\n",
       "357     Operasi SAR Lion Air, Basarnas: 56 Kantong Jen...\n",
       "595     Ketenangan serta kesabaran yang akan membuat k...\n",
       "223     alhamdulillah sudah menemukan Black Box Lion A...\n",
       "329     Turut berduka atas musibah dunia penerbangan i...\n",
       "548     Ternyata pak Kapolda Jabar bersama Kakor Brimo...\n",
       "490     Segenap keluarga besar KML Food turut berduka ...\n",
       "855     @jokowi Turut berduka cita atas tragedi Pesawa...\n",
       "482     Tragedi #LionAircrash #JT610 mengingatkan kepa...\n",
       "                              ...                        \n",
       "646     End off month semangat akhir bulan dan semanga...\n",
       "831     ya Allah, teiris hati liat postingan mas kukuh...\n",
       "562     Yang perlu dibenahi adalah manajemen keselamat...\n",
       "1037    Ini bukan dari dalam.pesawat lion air JT 610. ...\n",
       "686     Dari kmren browsing tentang kejadian kecelakaa...\n",
       "957     Basarnas dibantu TNI dan Polri untuk segera me...\n",
       "189     \"semoga Sang Pencipta memberikan tempat terbai...\n",
       "975     Pemerintah masih fokus pada pencarian dan peny...\n",
       "699     Presiden memastikan bahwa tim gabungan akan be...\n",
       "510     Pelindo II Kerahkan 2 Kapal Untuk Bantu Cari K...\n",
       "474     Di kampus masih banyak yg pakai pita hitam di ...\n",
       "856     Semoga keluarga yg di tinggalkan di beri ketab...\n",
       "747     #PrayForJT610 #PersibDay #Forever100DMP Menjel...\n",
       "252     Kami Segenap Keluarga Besar Direktorat Kelaiku...\n",
       "21      Duka jatuhnya pesawat Lion Air JT 610 belum us...\n",
       "459     Kantor KSOP Marunda menerima kunjungan &amp; m...\n",
       "276     Tim penyelam juga dikerahkan untuk menyelamatk...\n",
       "955     Semoga roh mereka yg meninggal dicucuri rahmat...\n",
       "385     InsyaAllah besok ada titik terang.Amiin.\\n#Pra...\n",
       "805         Stop operasi Lion air #LionAir #PrayForJT610'\n",
       "343     @jokowi Saya pribadi, menyampaikan Turut Berdu...\n",
       "769     Selain dapat menolong sesama, #donordarah juga...\n",
       "130     #BreakingNews Simpati dan doa dilakukan oleh a...\n",
       "871     Mengenal lebih dekat BSG, satuan khusus bentuk...\n",
       "87      Setiap nonton Youtube ttg Lion air JT 610 , se...\n",
       "330     Kotak hitam pesawat Lion Air PK-LQP JT 610 yan...\n",
       "466     Turut berduka cita atas musibah yang menimpa p...\n",
       "121     Saya sebagai mahasiswa aviasi kecewa dan meras...\n",
       "1044    Yang udah dapet berita turbulensi di dalam pes...\n",
       "860     Kalao dipikir lokasi jatuhnya pesawat sudah di...\n",
       "Name: tweets, Length: 711, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31      Abis baca berita, tim penyelam #LionAirJT610 d...\n",
       "413     #JT610\\nTerakhir jadi satu sama kak mery sama ...\n",
       "536                         Sudah dua hari #PrayForJT610'\n",
       "960     Pemerintah lakukan upaya terbaik untuk menemuk...\n",
       "793     Turut Berduka Cita #PrayForJT610 #AllahTheMerc...\n",
       "740     MENGENAL KOTAK HITAM\\n\\nUntuk mengungkap penye...\n",
       "950     Presiden jokowi sudah perintahkan basarnas. TN...\n",
       "721     Di luar agenda kepresidenan, Presiden Jokowi p...\n",
       "86      Baper gara\" JT 610 \\xf0\\x9f\\x98\\xad #PrayForLi...\n",
       "876     nggak hanya langit yang mendung hari ini,hati ...\n",
       "113     Presiden @jokowi tinjau Posko Evakuasi terpadu...\n",
       "1056    Doa kita untuk #JT610, semoga amal i an diteri...\n",
       "1033    Hanya doa yang bisa kita jadikan jalan untuk m...\n",
       "784     Semoga diberi kemudahan untuk pencaharian dan ...\n",
       "306     @mtrmkg10 Yamaha sama #PrayForJT610 apa hubung...\n",
       "1040    Semoga badan pesawat dan para korban bisa sege...\n",
       "885     Basarnas Berhasil Menemukan 26 Jenazah Korban ...\n",
       "425     #PrayForJT610 tetap di beri ketabahan para ahl...\n",
       "634     Pesawat Lion Air JT-610 jatuh di Perairan Kara...\n",
       "442     \"Dari KRI Rigel termasuk dari geosurvei saat i...\n",
       "796     @jokowi Inallilahi wainailaihi rojiun.... Tuha...\n",
       "323     Breaking Nyus...\\nYa Allah...terima kasih kota...\n",
       "675     Turut Berduka Cita atas musibah jatuhnya pesaw...\n",
       "643     Buntut kecelakaan Lion Air JT 610, Kemenhub in...\n",
       "890     Pastinya bahagia,ketika masih punya kesempatan...\n",
       "514     Ia duduk sejajar dengan warganya. Ia seorang a...\n",
       "333     Nunggu masuk \"air crash investigation\" nat.geo...\n",
       "260     Sedangkan tim dua bergerak ke arah Karawang ya...\n",
       "650     Saat petugas mengangkat beg berisi mayat merek...\n",
       "436     Kotak hitam menjadi benda yang paling diburu k...\n",
       "                              ...                        \n",
       "310     Semoga mereka selamat sampai tujuan, aman dala...\n",
       "499     Segenap Redaksi Radio Pelita Kasih Jakarta, tu...\n",
       "104     3 Jenazah Korban Lion Air yang Teridentifikasi...\n",
       "97                   Kita kembali berduka.. #PrayForJT610\n",
       "332                          Kotak Hitam sudah di temukan\n",
       "457     Teruntai do\\xe2\\x80\\x99a yang terbaik untuk Ib...\n",
       "314     Detik Deyik Pengangkatan Kotak Hitam Lion Air ...\n",
       "760        b\"@awanmaIam Masih ingat 'Skandal Ban bekas' ?\n",
       "953     Segenap dewan redaksi #MuslimMudaIndonesia tur...\n",
       "281     @IpungLombok halah embuh cak..... repot ngomon...\n",
       "266     #PrayForJT610 Percaya pd takdir Tuhan wajib hu...\n",
       "516     Turut berduka cita atas kecelakaan yang menimp...\n",
       "730     Dirinya juga sempat memberi semangat dan berko...\n",
       "535     Bupati Klaten @YaniSunarno\\nmenjenguk keluarga...\n",
       "578                          Kudu kuat \\n\\n#PrayForJT610'\n",
       "961     Kita berharap para keluarga korban bisa tenang...\n",
       "858     @jokowi ituuu yg benar bgtu. Semoga di tangan ...\n",
       "118     b\"Malam ini ada do'a bersama (lagi) seluruh ka...\n",
       "777     Kesedihan mendalam keluarga Rudi Lumbantoruan,...\n",
       "250     Kapal Baruna Jaya Siaga 24 Jam, Alhamdulillah ...\n",
       "424     Innalillahi....Jannatun Cintya Dewi, PNS KESDM...\n",
       "285     Pertamina turut berbela sungkawa atas musibah ...\n",
       "409     21:10 WIB Salah satu penemuan siang tadi. Seca...\n",
       "9       Relawan penyelam Basarnas, Syachrul Anto (48) ...\n",
       "467     @NasDem kita semua mendoakan yg terbaik bagi k...\n",
       "196     Tuhan tidak marah, beliau sayang kpd kita. jan...\n",
       "665     Megadeth Lelang Gitar Legendaris Untuk Korban ...\n",
       "81      Mesin yg sangat jauh berbeda antara LEAP-1B (B...\n",
       "1058    #PrayForJT610 semoga tetap ada harapan di teng...\n",
       "974     Pemeritah terus melakukan operasi pencarian da...\n",
       "Name: tweets, Length: 351, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "norm_train_corpus = normalize_corpus(train_corpus,\n",
    "                                      lemmatize=True,\n",
    "                                      only_text_chars=True)\n",
    "norm_test_corpus = normalize_corpus(test_corpus,\n",
    "                                      lemmatize=True,\n",
    "                                      only_text_chars=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tfidf_transformer(bow_matrix):\n",
    "    transformer = TfidfTransformer(norm='l2',\n",
    "                                  smooth_idf=True,\n",
    "                                  use_idf=True)\n",
    "    tfidf_matrix = transformer.fit_transform(bow_matrix)\n",
    "    return transformer, tfidf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tfidf_extractor(corpus, ngram_range=(1,1)):\n",
    "    vectorizer = TfidfVectorizer(min_df=1,\n",
    "                                norm='l2',\n",
    "                                smooth_idf=True,\n",
    "                                use_idf=True,\n",
    "                                ngram_range=ngram_range)\n",
    "    features = vectorizer.fit_transform(corpus)\n",
    "    return vectorizer, features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tfidf_vectorizer, tfidf_train_features = tfidf_extractor(norm_train_corpus)\n",
    "tfidf_test_features = tfidf_vectorizer.transform(norm_test_corpus)\n",
    "\n",
    "tokenized_train = [nltk.word_tokenize(text)\n",
    "                  for text in norm_train_corpus]\n",
    "tokenized_test = [nltk.word_tokenize(text)\n",
    "                 for text in norm_test_corpus]\n",
    "\n",
    "model = gensim.models.Word2Vec(tokenized_train,\n",
    "                              size=500,\n",
    "                              window=100,\n",
    "                              min_count=30,\n",
    "                              sample=1e-3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "\n",
    "def get_metrics(true_labels, predicted_labels):\n",
    "    print('Accuracy: ', np.round(metrics.accuracy_score(true_labels,\n",
    "                                                     predicted_labels),2))\n",
    "    print('Precision: ', np.round(metrics.precision_score(true_labels,\n",
    "                                                     predicted_labels,\n",
    "                                                        average='weighted'),2))\n",
    "    print('Recall: ', np.round(metrics.recall_score(true_labels,\n",
    "                                                     predicted_labels,\n",
    "                                                        average='weighted'),2))\n",
    "    print('F1 Score: ', np.round(metrics.f1_score(true_labels,\n",
    "                                                     predicted_labels,\n",
    "                                                        average='weighted'),2))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#def train_predict_evaluate_model(classifier,\n",
    "#                                train_features, train_labels,\n",
    "#                                test_features, test_labels):\n",
    "#    classifier.fit(train_features, train_labels)\n",
    "#    predictions = classifier.predict(test_features)\n",
    "#    get_metrics(true_labels=test_labels,\n",
    "#               predicted_labels=predictions)\n",
    "#    return predictions#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ACER\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label test: 31     positif\n",
      "413    positif\n",
      "536     netral\n",
      "960    positif\n",
      "793    positif\n",
      "740     netral\n",
      "950     netral\n",
      "721     netral\n",
      "86      netral\n",
      "876     netral\n",
      "Name: label, dtype: object\n",
      "Prediction test: ['netral' 'netral' 'netral' 'positif' 'positif']\n",
      "Accuration: 0.6182336182336182\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "train_features=tfidf_train_features\n",
    "train_labels=train_labels\n",
    "test_features=tfidf_test_features\n",
    "test_labels=test_labels\n",
    "\n",
    "clsfr = SGDClassifier(loss='hinge', n_iter=100)\n",
    "clsfr.fit(train_features, train_labels)\n",
    "predictions = clsfr.predict(test_features)\n",
    "    \n",
    "print(\"Label test: \" +format(test_labels[:10]))\n",
    "print(\"Prediction test: \"+ format(predictions[:5]))\n",
    "print(\"Accuration: \"+format(clsfr.score(test_features,test_labels)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ACER\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6182336182336182\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "pipe = make_pipeline(tfidf_vectorizer,clsfr)\n",
    "pipe.fit(train_corpus,train_labels)\n",
    "\n",
    "print(pipe.score(test_corpus,test_labels))\n",
    "\n",
    "tempFeature=normalize_corpus(feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['E:\\\\SGDClassifierSentence.pkl']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tempData=pd.DataFrame(np.column_stack([feature,label]),columns=('Feature','Label'))\n",
    "tempData\n",
    "tempRest = [pipe,tempData]\n",
    "line = np.array(['Siapa aku ini? Yang bukan meenjadi siapa - siapa bagimu'])\n",
    "pipe.predict(line)\n",
    "joblibFile = \"E:\\SGDClassifierSentence.pkl\"\n",
    "from sklearn.externals import joblib\n",
    "joblib.dump(tempRest,joblibFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.62\n",
      "Precision:  0.61\n",
      "Recall:  0.62\n",
      "F1 Score:  0.61\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ACER\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\ACER\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1137: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "C:\\Users\\ACER\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\ACER\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "def get_metrics(true_labels, predicted_labels):\n",
    "    print('Accuracy: ', np.round(metrics.accuracy_score(true_labels,\n",
    "                                                     predicted_labels),2))\n",
    "    print('Precision: ', np.round(metrics.precision_score(true_labels,\n",
    "                                                     predicted_labels,\n",
    "                                                        average='weighted'),2))\n",
    "    print('Recall: ', np.round(metrics.recall_score(true_labels,\n",
    "                                                     predicted_labels,\n",
    "                                                        average='weighted'),2))\n",
    "    print('F1 Score: ', np.round(metrics.f1_score(true_labels,\n",
    "                                                     predicted_labels,\n",
    "                                                        average='weighted'),2))\n",
    "get_metrics(true_labels=test_labels,predicted_labels=predictions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
